## 信息检索作业报告4：web搜索设计

本次作业要求: 

（1）爬取网站内容

（2）链接分析，构建索引

（3）设计查询

（4）web网站设计

### 爬取网页内容

这次作业中我使用了scrapy来实现南开新闻页面的网页内容获取。

为了获取网页页面内容，我分两步来实现页面的爬取。第一步：在南开新闻网的目录页面抓取到所有的url来存储起来。

scrapy的页面获取通过继承scrapy的spider来实现：

~~~python
import scrapy

import re
import os
from lxml import etree

from newsurls.items import urlsItem

#path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class URLnews(scrapy.Spider):
    name = 'links'
    temp = []
    for i in range(1,800):
        temp.append("http://news.nankai.edu.cn/mtnk/system/count//0006000/000000000000/000/000/c0006000000000000000_000000{}.shtml".format(i))
    temp.append("http://news.nankai.edu.cn/ywsd/index.shtml")
    start_urls = temp
    def parse(self,response):
        item = urlsItem()
        sel = scrapy.Selector(response)
        find = response.xpath("/html/body/div/table[3]/tbody/tr/td[1]/table[2]/tbody/tr/td")
        t = find.xpath("//a/@href").extract()
        t = t[11:-6]
        t.remove('http://news.nankai.edu.cn/wx/index.shtml')
        item['links'] = t
        yield item
        
~~~

使用上面的代码，可以遍历所有的目录页面，用xpath方法得到目录中所有的url，然后对获得的url做一些处理，将重复的、没用的url删除，就可以得到最后的内容，执行这条代码``cmdline.execute("scrapy crawl news -o news2.json -s FEED_EXPORT_ENCODING=UTF-8".split())`` 就可以运行爬虫，将最后的结果下载下来。

在得到上面的所有目标页面的url之后，利用这些url来实现页面内容的抓取。

~~~python
import scrapy
import re
import os
from lxml import etree
path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from nknews.items import NknewsItem
import json
with open('F:\\Scrapy\\nknews\\nknews\\spiders\\urls2.json','r',encoding='utf-8')as fp:
    json_data = json.load(fp)
ret = []
for i in range(801):
    ret+=(json_data[i]['links'])
class NKnews(scrapy.Spider):
    name = 'news'
    start_urls = ret
    def parse(self,response):
        item = NknewsItem()
        sel = scrapy.Selector(response)
        news = sel.xpath("//html/body/div/table[3]")
        t = []
        item['content'] = response.xpath("//p/text()").extract()
        #item['content'] = news.xpath("//tbody/tr/td[1]/table[2]/tbody/tr[3]/text()").extract()
        item['links'] = news.xpath("//a/@href").extract()
        t.append(str(response)[5:-1])
        item['url'] = t
        yield item
~~~

这里设计item包括内容，对应链接和该网页的url。

### 链接分析

使用pagerank算法来进行分析，针对网页之间的链接关系来计算网页的评分。

PageRank算法总的来说就是预先给每个网页一个PR值（下面用PR值指代PageRank值），PR值物理意义上为一个网页被访问概率。另外，一般情况下，所有网页的PR值的总和为1。如果不为1的话也不是不行，最后算出来的不同网页之间PR值的大小关系仍然是正确的，只是不能直接地反映概率了。

预先给定PR值后，通过下面的算法不断迭代，直至达到平稳分布为止。
$$
PR(p_i)=\alpha \sum PR(p_j)/L(p_j)+(1-\alpha)/N
$$
我们可以用一个矩阵来表示这张图的出链入链关系，Sij=0 Sij=0表示jj网页没有对ii网页的出链：

![image-20211211194012111](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211211194012111.png)

定义矩阵：
$$
A=αS+\frac{(1−α)}{N}eeT
$$

$$
P_{n+1}=AP_n
$$

计算pr就是不停的迭代收敛。

在算法的实现中，建立一个矩阵来表现网页之间的链接关系，然后按照上面的矩阵关系来实现迭代：

~~~pyt
import json
import numpy as np
with open('./nknews/nknews/news.json','r',encoding='utf-8')as fp:
    json_data = json.load(fp)
len = len(json_data)

mat = np.zeros((len,len))
for i in range(len):
    for j in range(len):
        if json_data[i]['url'][0] in json_data[j]['links']:
            mat[i][j] = 1
def GtoM(G, N):
    M = np.zeros((N, N))
    D = np.sum(G,axis=0)
    for i in range(N):
        for j in range(N):
            if D[j] == 0:
                continue
            M[i][j] = G[i][j] / D[j] # watch out! M_j_i instead of M_i_j
    return M
def PageRank(M, N, T=30000, eps=1e-6, beta=0.8):
    R = np.ones(N) / N
    teleport = np.ones(N) / N
    for time in range(T):
        R_new = beta * np.dot(M, R) + (1-beta)*teleport
        if np.linalg.norm(R_new - R) < eps:
            break
        R = R_new.copy()
    return R_new
~~~

当迭代到eps到达一个值之后就可以停止迭代了。

~~~python
ans = GtoM(mat,len)
rank = PageRank(ans,len)
so=np.argsort(rank)
with open('./nknews/nknews/news.json','w') as f:
    json.dump(json_data,f)
~~~

进行pagerank之后进行排序，将排序结果作为一个索引存入json数据包

将json数据直接作为索引存入es引擎中。然后调用es的api来进行查询。

### 页面设计

![image-20211211194719564](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211211194719564.png)

登上服务器，首先跳出的是一个登录页面，在es数据库中已经存入了用户名和密码。如果输入正确的用户名和密码，就可以进入搜索页面了。如果输入错误，也可以进入搜索页面，但是只能使用最基础的搜索功能。

![image-20211211194854776](https://s2.loli.net/2021/12/11/pz9uNEWmcP67JBO.png)

最基础的搜索页面如图，点击搜索就可以了。

![image-20211211195438437](https://s2.loli.net/2021/12/11/bN5wqCR1TUmZJ8z.png)

这里在后台直接调用了elastic search api来进行搜索。es数据库使用的是bm25算法（以前使用的是向量空间模型），这里直接使用content内容匹配，field_value_factor是按照pagerank的影响权重，用到了链接分析的成果。

BM25算法的公式主要有以下三个方面组成。

~~~python
query中每个单词t与文档d之间的相关性
单词t与query之间的相似性
每个单词的权重
~~~

返回查询前三的结果：

![image-20211211194931864](https://s2.loli.net/2021/12/11/duDxQs2EPMbI8y3.png)

搜索得到的结果包含基本的页面快照（正文页面内容），还有对应的链接，点击下面的链接就可以达到目标位置。

![image-20211211195748009](https://s2.loli.net/2021/12/11/KZotkxiNTjcqImb.png)

如果当时正确登录，就可以进入高级搜索的页面。

![image-20211211195840710](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211211195840710.png)

这里的查询功能包括必要元素查询，可选关键字查询还有html的内容查询。因为html里面的信息一般包括了日期或者其他的一些信息，所以也可以把它作为高级查询的元素。

左上角的可选框可以选择普通查询和短语查询，分别对应es后台的match查询和match_phrase查询，这也是高级查询的一个功能。

![image-20211211200152321](https://s2.loli.net/2021/12/11/tMH9g4EezK8ZfhT.png)

如果搜索输入内容，使用es数据库后台查询没有结果，则会跳转到一个错误页面：

![image-20211211200426846](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211211200426846.png)

查询记录保存：

![image-20211211200540604](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211211200540604.png)

浏览器可以帮你保存搜索过的内容。

### 个性化搜索

因为我们在这里已经实现了用户登录的功能，因此将用户的查询内容和页面搜索结合，可以很容易地实现在每次查询的时候更新用户的查询信息。

![image-20211211200825099](C:\Users\kentl\AppData\Roaming\Typora\typora-user-images\image-20211211200825099.png)

在es的数据库的账户索引中设计一个字段来存储所有的搜索记录。

在es的news索引可以增加字段，用类似上面链接分析的方法增加该用户的查询次数的权重来进行排序。实现针对某个用户的个性化查询排序。



